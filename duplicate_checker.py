"""
duplicate_checker.py

CÃ´ng cá»¥ Streamlit Ä‘á»ƒ kiá»ƒm tra trÃ¹ng há»“ sÆ¡ trÆ°á»›c khi phÃª duyá»‡t.
DÃ nh cho dá»¯ liá»‡u Äáº¤T á» (Land) tá»« Excel iFast.

CÃ¡c nhÃ³m kiá»ƒm tra trÃ¹ng:

A. PhÃª duyá»‡t vs HoÃ n thÃ nh
--------------------------
1) TRÃ™NG Tá»ŒA Äá»˜ (Æ°u tiÃªn, cháº¯c cháº¯n):
   - Tá»a Ä‘á»™ chuáº©n hÃ³a (coord_norm) trÃ¹ng nhau
   - 'Thá»i Ä‘iá»ƒm thu tháº­p thÃ´ng tin' cá»§a há»“ sÆ¡ PhÃª duyá»‡t > há»“ sÆ¡ HoÃ n thÃ nh
   â†’ LuÃ´n gáº¯n nhÃ£n: Cáº¢NH BÃO TRÃ™NG (khÃ´ng xÃ©t NgÆ°á»i táº¡o)
   â†’ Cá»™t hiá»ƒn thá»‹: Tá»ŒA Äá»˜

2) TRÃ™NG Äá»ŠA CHá»ˆ (5 cá»™t W,X,Y,Z,AE):
   - TrÃ¹ng 5 thÃ´ng tin:
        Tá»‰nh/ThÃ nh phá»‘
        Quáº­n/Huyá»‡n/Thá»‹ xÃ£
        XÃ£/PhÆ°á»ng
        ÄÆ°á»ng/Phá»‘
        Sá»‘ nhÃ 
   - 'Thá»i Ä‘iá»ƒm thu tháº­p thÃ´ng tin' cá»§a há»“ sÆ¡ PhÃª duyá»‡t > há»“ sÆ¡ HoÃ n thÃ nh
   - Náº¿u cÃ¹ng NgÆ°á»i táº¡o  â†’ Cáº¢NH BÃO TRÃ™NG
   - Náº¿u khÃ¡c NgÆ°á»i táº¡o â†’ NGHI NGá»œ TRÃ™NG
   â†’ Cá»™t hiá»ƒn thá»‹: Äá»ŠA CHá»ˆ (náº¿u khÃ´ng cÃ³ trÃ¹ng tá»a Ä‘á»™)

B. HoÃ n thÃ nh vs HoÃ n thÃ nh
---------------------------
- So sÃ¡nh cÃ¡c há»“ sÆ¡ Ä‘á»u á»Ÿ tráº¡ng thÃ¡i 'HoÃ n thÃ nh' vá»›i nhau
- Chá»‰ xÃ©t cÃ¡c há»“ sÆ¡ HoÃ n thÃ nh cÃ³ 'Thá»i Ä‘iá»ƒm thu tháº­p thÃ´ng tin' nhá» hÆ¡n há»“ sÆ¡ Ä‘ang xÃ©t
- Rule giá»‘ng pháº§n A:
    + TrÃ¹ng tá»a Ä‘á»™ â†’ Cáº¢NH BÃO TRÃ™NG
    + TrÃ¹ng Ä‘á»‹a chá»‰:
        * CÃ¹ng NgÆ°á»i táº¡o â†’ Cáº¢NH BÃO TRÃ™NG
        * KhÃ¡c NgÆ°á»i táº¡o â†’ NGHI NGá»œ TRÃ™NG

Output chung cho cáº£ hai nhÃ³m:
- ID                : ID cá»§a há»“ sÆ¡ bá»‹ coi lÃ  trÃ¹ng (há»“ sÆ¡ vá» sau)
- NgÆ°á»i táº¡o         : NgÆ°á»i táº¡o cá»§a há»“ sÆ¡ Ä‘Ã³
- LÃ½ do trÃ¹ng       : Cáº£nh bÃ¡o / Nghi ngá» + mÃ´ táº£ chi tiáº¿t
- Äá»‹a chá»‰/Tá»a Ä‘á»™ trÃ¹ng:
    + Náº¿u cÃ³ trÃ¹ng tá»a Ä‘á»™ â†’ chá»‰ hiá»ƒn thá»‹ tá»a Ä‘á»™ (cá»™t AF)
    + Náº¿u chá»‰ trÃ¹ng Ä‘á»‹a chá»‰ â†’ hiá»ƒn thá»‹ Äá»‹a chá»‰: Sá»‘ nhÃ  â€“ ÄÆ°á»ng â€“ XÃ£ â€“ Quáº­n â€“ Tá»‰nh
- ID trÃ¹ng          : cÃ¡c ID trÆ°á»›c Ä‘Ã³ mÃ  há»“ sÆ¡ nÃ y trÃ¹ng (ngÄƒn cÃ¡ch '; ')
- NgÆ°á»i táº¡o trÃ¹ng   : NgÆ°á»i táº¡o tÆ°Æ¡ng á»©ng cÃ¡c ID trÃ¹ng
"""

from __future__ import annotations

from typing import Optional, List, Dict, Any, Set
import io

import pandas as pd

try:
    import streamlit as st  # type: ignore
except ImportError:  # pragma: no cover
    st = None  # type: ignore


# ==========================
#  Constants
# ==========================

ADDR_COLS = [
    "Tá»‰nh/ThÃ nh phá»‘",       # W
    "Quáº­n/Huyá»‡n/Thá»‹ xÃ£",    # X
    "XÃ£/PhÆ°á»ng",            # Y
    "ÄÆ°á»ng/Phá»‘",            # Z
    "Sá»‘ nhÃ ",               # AE
]

CREATOR_COL = "NgÆ°á»i táº¡o"                     # cá»™t E
TIME_COL = "Thá»i Ä‘iá»ƒm thu tháº­p thÃ´ng tin"     # cá»™t L
COORD_COL = "Tá»a Ä‘á»™"                          # cá»™t AF
STATUS_COL = "Giai Ä‘oáº¡n hiá»‡n táº¡i"             # cá»™t H
ID_COL = "ID"


# ==========================
#  Helpers
# ==========================

def build_addr_key(row: pd.Series) -> str:
    parts = [str(row.get(col, "")).strip().lower() for col in ADDR_COLS]
    return "||".join(parts)


def normalize_coord(value: Any, max_len: int = 8) -> Optional[str]:
    """Chuáº©n hÃ³a tá»a Ä‘á»™ Ä‘á»ƒ báº¯t cáº£ case thÃªm sá»‘ láº» phÃ­a sau."""
    if pd.isna(value):
        return None
    try:
        lat_raw, lon_raw = str(value).split(",")
        lat = lat_raw.strip()[:max_len]
        lon = lon_raw.strip()[:max_len]
        return f"{lat},{lon}"
    except Exception:
        return None


def format_address(row: pd.Series) -> str:
    """Sá»‘ nhÃ  â€“ ÄÆ°á»ng/Phá»‘ â€“ XÃ£/PhÆ°á»ng â€“ Quáº­n/Huyá»‡n/Thá»‹ xÃ£ â€“ Tá»‰nh/ThÃ nh phá»‘."""
    parts = [
        str(row.get("Sá»‘ nhÃ ", "")).strip(),
        str(row.get("ÄÆ°á»ng/Phá»‘", "")).strip(),
        str(row.get("XÃ£/PhÆ°á»ng", "")).strip(),
        str(row.get("Quáº­n/Huyá»‡n/Thá»‹ xÃ£", "")).strip(),
        str(row.get("Tá»‰nh/ThÃ nh phá»‘", "")).strip(),
    ]
    return " â€“ ".join([p for p in parts if p])


# ==========================
#  Core Logic
# ==========================

def _prepare(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """Chuáº©n hÃ³a chung: key Ä‘á»‹a chá»‰, tá»a Ä‘á»™, thá»i gian; tÃ¡ch nhÃ³m tráº¡ng thÃ¡i."""
    # check required columns
    required_cols = ADDR_COLS + [
        CREATOR_COL, TIME_COL, COORD_COL, STATUS_COL, ID_COL
    ]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Thiáº¿u cá»™t: {col}")

    df = df.copy()

    # chuáº©n hÃ³a
    df["addr_key"] = df.apply(build_addr_key, axis=1)
    df["coord_norm"] = df[COORD_COL].apply(normalize_coord)
    df["time_norm"] = pd.to_datetime(df[TIME_COL], dayfirst=True, errors="coerce")
    df["creator_norm"] = df[CREATOR_COL].astype(str).str.strip()

    hoan_thanh = df[df[STATUS_COL] == "HoÃ n thÃ nh"].copy()
    phe_duyet = df[df[STATUS_COL] == "PhÃª duyá»‡t"].copy()

    return {
        "all": df,
        "hoan_thanh": hoan_thanh,
        "phe_duyet": phe_duyet,
    }


def _build_groups(hoan_thanh: pd.DataFrame):
    """Táº¡o group cho HoÃ n thÃ nh Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng."""
    addr_groups = hoan_thanh.groupby("addr_key").groups
    coord_groups = hoan_thanh.groupby("coord_norm").groups
    return addr_groups, coord_groups


def _collect_result(
    row: pd.Series,
    duplicate_ids: Set[Any],
    duplicate_creators: Set[str],
    has_coord_dup: bool,
    has_addr_dup: bool,
    severity_label: str,
    reason_details: List[str],
) -> Dict[str, Any]:
    # Æ°u tiÃªn hiá»ƒn thá»‹ tá»a Ä‘á»™ náº¿u cÃ³ trÃ¹ng tá»a Ä‘á»™
    if has_coord_dup:
        info = f"Tá»a Ä‘á»™: {row.get(COORD_COL, '')}"
    elif has_addr_dup:
        info = f"Äá»‹a chá»‰: {format_address(row)}"
    else:
        info = ""

    return {
        "ID": row.get(ID_COL),
        "NgÆ°á»i táº¡o": row.get("creator_norm", ""),
        "LÃ½ do trÃ¹ng": f"{severity_label} â€“ " + " ; ".join(reason_details),
        "Äá»‹a chá»‰/Tá»a Ä‘á»™ trÃ¹ng": info,
        "ID trÃ¹ng": "; ".join(str(x) for x in sorted(duplicate_ids)),
        "NgÆ°á»i táº¡o trÃ¹ng": "; ".join(sorted(duplicate_creators)),
    }


def check_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    Tráº£ vá» báº£ng trÃ¹ng bao gá»“m:
    - PhÃª duyá»‡t vs HoÃ n thÃ nh
    - HoÃ n thÃ nh vs HoÃ n thÃ nh
    """
    prep = _prepare(df)
    hoan_thanh = prep["hoan_thanh"]
    phe_duyet = prep["phe_duyet"]

    addr_groups, coord_groups = _build_groups(hoan_thanh)

    results: List[Dict[str, Any]] = []

    # ==========================
    # A. PhÃª duyá»‡t vs HoÃ n thÃ nh
    # ==========================
    for _, row in phe_duyet.iterrows():
        duplicate_ids: Set[Any] = set()
        duplicate_creators: Set[str] = set()
        reason_details: List[str] = []
        severity: Optional[str] = None

        row_time = row["time_norm"]
        addr_key = row["addr_key"]
        coord_key = row["coord_norm"]
        creator = row["creator_norm"]

        has_addr_dup = False
        has_coord_dup = False

        # ---- TrÃ¹ng Ä‘á»‹a chá»‰
        addr_idx = addr_groups.get(addr_key)
        if addr_idx is not None and len(addr_idx) > 0:
            subset = hoan_thanh.loc[addr_idx]
            subset = subset[subset["time_norm"] < row_time]

            if not subset.empty:
                has_addr_dup = True
                same_creator = subset[subset["creator_norm"] == creator]
                diff_creator = subset[subset["creator_norm"] != creator]

                if not same_creator.empty:
                    severity = "Cáº£nh bÃ¡o trÃ¹ng"
                    duplicate_ids.update(same_creator[ID_COL])
                    duplicate_creators.update(same_creator["creator_norm"])
                    reason_details.append(
                        "PhÃª duyá»‡t vs HoÃ n thÃ nh â€“ CÃ¹ng NgÆ°á»i táº¡o vÃ  trÃ¹ng 5 thÃ´ng tin Ä‘á»‹a chá»‰"
                    )

                if not diff_creator.empty:
                    if severity is None:
                        severity = "Nghi ngá» trÃ¹ng"
                    duplicate_ids.update(diff_creator[ID_COL])
                    duplicate_creators.update(diff_creator["creator_norm"])
                    reason_details.append(
                        "PhÃª duyá»‡t vs HoÃ n thÃ nh â€“ KhÃ¡c NgÆ°á»i táº¡o nhÆ°ng trÃ¹ng 5 thÃ´ng tin Ä‘á»‹a chá»‰"
                    )

        # ---- TrÃ¹ng tá»a Ä‘á»™
        coord_idx = coord_groups.get(coord_key)
        if coord_idx is not None and len(coord_idx) > 0:
            subset = hoan_thanh.loc[coord_idx]
            subset = subset[subset["time_norm"] < row_time]

            if not subset.empty:
                has_coord_dup = True
                severity = "Cáº£nh bÃ¡o trÃ¹ng"
                duplicate_ids.update(subset[ID_COL])
                duplicate_creators.update(subset["creator_norm"])
                reason_details.append(
                    "PhÃª duyá»‡t vs HoÃ n thÃ nh â€“ TrÃ¹ng tá»a Ä‘á»™ (100% hoáº·c gáº§n Ä‘Ãºng)"
                )

        if duplicate_ids:
            if severity is None:
                severity = "Nghi ngá» trÃ¹ng"  # fallback, vá» lÃ½ thuyáº¿t khÃ´ng xáº£y ra
            results.append(
                _collect_result(
                    row=row,
                    duplicate_ids=duplicate_ids,
                    duplicate_creators=duplicate_creators,
                    has_coord_dup=has_coord_dup,
                    has_addr_dup=has_addr_dup,
                    severity_label=severity,
                    reason_details=reason_details,
                )
            )

    # ==========================
    # B. HoÃ n thÃ nh vs HoÃ n thÃ nh
    # ==========================
    for _, row in hoan_thanh.iterrows():
        duplicate_ids: Set[Any] = set()
        duplicate_creators: Set[str] = set()
        reason_details: List[str] = []
        severity: Optional[str] = None

        row_time = row["time_norm"]
        addr_key = row["addr_key"]
        coord_key = row["coord_norm"]
        creator = row["creator_norm"]
        row_id = row[ID_COL]

        has_addr_dup = False
        has_coord_dup = False

        # ---- TrÃ¹ng Ä‘á»‹a chá»‰ giá»¯a HoÃ n thÃ nh vá»›i nhau
        addr_idx = addr_groups.get(addr_key)
        if addr_idx is not None and len(addr_idx) > 0:
            subset = hoan_thanh.loc[addr_idx]
            subset = subset[(subset["time_norm"] < row_time) & (subset[ID_COL] != row_id)]

            if not subset.empty:
                has_addr_dup = True
                same_creator = subset[subset["creator_norm"] == creator]
                diff_creator = subset[subset["creator_norm"] != creator]

                if not same_creator.empty:
                    severity = "Cáº£nh bÃ¡o trÃ¹ng"
                    duplicate_ids.update(same_creator[ID_COL])
                    duplicate_creators.update(same_creator["creator_norm"])
                    reason_details.append(
                        "HoÃ n thÃ nh vs HoÃ n thÃ nh â€“ CÃ¹ng NgÆ°á»i táº¡o vÃ  trÃ¹ng 5 thÃ´ng tin Ä‘á»‹a chá»‰"
                    )

                if not diff_creator.empty:
                    if severity is None:
                        severity = "Nghi ngá» trÃ¹ng"
                    duplicate_ids.update(diff_creator[ID_COL])
                    duplicate_creators.update(diff_creator["creator_norm"])
                    reason_details.append(
                        "HoÃ n thÃ nh vs HoÃ n thÃ nh â€“ KhÃ¡c NgÆ°á»i táº¡o nhÆ°ng trÃ¹ng 5 thÃ´ng tin Ä‘á»‹a chá»‰"
                    )

        # ---- TrÃ¹ng tá»a Ä‘á»™ giá»¯a HoÃ n thÃ nh vá»›i nhau
        coord_idx = coord_groups.get(coord_key)
        if coord_idx is not None and len(coord_idx) > 0:
            subset = hoan_thanh.loc[coord_idx]
            subset = subset[(subset["time_norm"] < row_time) & (subset[ID_COL] != row_id)]

            if not subset.empty:
                has_coord_dup = True
                severity = "Cáº£nh bÃ¡o trÃ¹ng"
                duplicate_ids.update(subset[ID_COL])
                duplicate_creators.update(subset["creator_norm"])
                reason_details.append(
                    "HoÃ n thÃ nh vs HoÃ n thÃ nh â€“ TrÃ¹ng tá»a Ä‘á»™ (100% hoáº·c gáº§n Ä‘Ãºng)"
                )

        if duplicate_ids:
            if severity is None:
                severity = "Nghi ngá» trÃ¹ng"
            results.append(
                _collect_result(
                    row=row,
                    duplicate_ids=duplicate_ids,
                    duplicate_creators=duplicate_creators,
                    has_coord_dup=has_coord_dup,
                    has_addr_dup=has_addr_dup,
                    severity_label=severity,
                    reason_details=reason_details,
                )
            )

    return pd.DataFrame(results)


# ==========================
#  Streamlit App
# ==========================

def run_app() -> None:  # pragma: no cover
    if st is None:
        raise RuntimeError("Streamlit chÆ°a Ä‘Æ°á»£c cÃ i. Cháº¡y: pip install streamlit")

    st.set_page_config(page_title="iFast Duplicate Checker", layout="wide")
    st.title("ğŸ§® iFast â€“ CÃ´ng cá»¥ kiá»ƒm tra trÃ¹ng há»“ sÆ¡")

    st.markdown(
        """
        CÃ´ng cá»¥ kiá»ƒm tra trÃ¹ng **há»“ sÆ¡ Äáº¥t á»Ÿ** trong iFast.

        **NhÃ³m kiá»ƒm tra:**
        - PhÃª duyá»‡t vs HoÃ n thÃ nh (há»“ sÆ¡ Ä‘ang trÃ¬nh so vá»›i há»“ sÆ¡ Ä‘Ã£ hoÃ n thÃ nh)
        - HoÃ n thÃ nh vs HoÃ n thÃ nh (cÃ¡c há»“ sÆ¡ Ä‘Ã£ hoÃ n thÃ nh trÃ¹ng nhau)

        **Æ¯u tiÃªn hiá»ƒn thá»‹:**
        - Náº¿u trÃ¹ng tá»a Ä‘á»™ â†’ chá»‰ hiá»ƒn thá»‹ tá»a Ä‘á»™
        - Náº¿u chá»‰ trÃ¹ng Ä‘á»‹a chá»‰ â†’ hiá»ƒn thá»‹ Ä‘á»‹a chá»‰
        """
    )

    uploaded = st.file_uploader("ğŸ“¥ Táº£i file Excel (.xlsx) xuáº¥t tá»« iFast", type=["xlsx"])
    if uploaded is None:
        st.info("Vui lÃ²ng táº£i lÃªn file Excel Ä‘á»ƒ báº¯t Ä‘áº§u kiá»ƒm tra.")
        return

    try:
        df = pd.read_excel(uploaded)
    except Exception as e:
        st.error(f"Lá»—i Ä‘á»c file Excel: {e}")
        return

    st.subheader("ğŸ” Xem trÆ°á»›c dá»¯ liá»‡u")
    with st.expander("Xem 5 dÃ²ng Ä‘áº§u"):
        st.dataframe(df.head())

    st.subheader("ğŸ“Š Káº¿t quáº£ kiá»ƒm tra trÃ¹ng")

    try:
        dup_df = check_duplicates(df)
    except Exception as e:
        st.error(f"Lá»—i khi kiá»ƒm tra trÃ¹ng: {e}")
        return

    if dup_df.empty:
        st.success("âœ… KhÃ´ng phÃ¡t hiá»‡n há»“ sÆ¡ trÃ¹ng hoáº·c nghi ngá» trÃ¹ng.")
    else:
        st.error(f"âš  PhÃ¡t hiá»‡n {len(dup_df)} há»“ sÆ¡ trÃ¹ng hoáº·c nghi ngá» trÃ¹ng.")
        st.dataframe(dup_df, use_container_width=True)

        # ===== Táº¢I Vá»€ DÆ¯á»šI Dáº NG EXCEL .XLSX =====
        output = io.BytesIO()
        # cáº§n thÆ° viá»‡n openpyxl trong requirements.txt
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            dup_df.to_excel(writer, index=False, sheet_name="Duplicates")
        output.seek(0)

        st.download_button(
            label="â¬‡ï¸ Táº£i danh sÃ¡ch trÃ¹ng (Excel)",
            data=output,
            file_name="detected_duplicates.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )


if __name__ == "__main__":  # pragma: no cover
    if st is not None:
        run_app()
    else:
        print("ÄÃ¢y lÃ  module cho Streamlit. Cháº¡y báº±ng:\n  streamlit run duplicate_checker.py")
