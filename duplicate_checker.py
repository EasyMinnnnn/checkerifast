"""
duplicate_checker.py
====================

This module contains a Streamlit application for checking duplicate property
records prior to approval.  It supports two types of checks: land (â€œÄáº¥t á»Ÿâ€) and
apartment (â€œCÄƒn há»™ chung cÆ°â€).  The current implementation focuses on the land
dataset and demonstrates how to identify records in the â€œPhÃª duyá»‡tâ€ (submitted
for approval) stage that duplicate previously approved records.

Usage
-----
Run the script with Streamlit:

```
streamlit run duplicate_checker.py
```

Upload the exported Excel file.  The application will display a table of
entries flagged as duplicates along with the reasons (address match, exact
coordinate match or approximate coordinate match).

Notes
-----
This script is designed to be hosted on a platform such as GitHub and
integrated with Streamlit.  It does not perform any network operations.
"""

from __future__ import annotations

import math
import re
from typing import Dict, List, Optional, Tuple

import pandas as pd

try:
    # Streamlit is only required when running the application.  Wrapping in
    # try/except allows the module (and its functions) to be imported in
    # environments where Streamlit isn't installed (e.g., during unit tests).
    import streamlit as st  # type: ignore
except ImportError:
    st = None  # type: ignore


def standardize_address(row: pd.Series) -> str:
    """Construct a normalized key from the five address columns.

    The address fields are converted to uppercase strings and stripped of
    leading/trailing whitespace before concatenation.  Missing values are
    converted to the string ``"nan"``.

    Parameters
    ----------
    row: pd.Series
        A row from the DataFrame containing the address fields.

    Returns
    -------
    str
        A concatenated address key used for exact matching.
    """
    cols = [
        "Tá»‰nh/ThÃ nh phá»‘",
        "Quáº­n/Huyá»‡n/Thá»‹ xÃ£",
        "XÃ£/PhÆ°á»ng",
        "ÄÆ°á»ng/Phá»‘",
        "Sá»‘ nhÃ ",
    ]
    values = []
    for col in cols:
        val = row.get(col, "")
        if pd.isna(val):
            val = "nan"
        val = str(val).strip().upper()
        values.append(val)
    return "||".join(values)


def parse_coords(coord: str) -> Optional[Tuple[float, float]]:
    """Parse a coordinate string into latitude and longitude floats.

    The input is expected to be in the form ``"lat,lon"`` with a comma
    separating the two values.  If parsing fails, ``None`` is returned.

    Parameters
    ----------
    coord: str
        A string representing a pair of latitude and longitude values.

    Returns
    -------
    Optional[Tuple[float, float]]
        A tuple ``(lat, lon)`` if parsing succeeds, otherwise ``None``.
    """
    if not coord or pd.isna(coord):
        return None
    coord = str(coord).strip()
    parts = coord.split(",")
    if len(parts) != 2:
        return None
    try:
        lat = float(parts[0])
        lon = float(parts[1])
        return lat, lon
    except ValueError:
        return None


def build_lookup(df: pd.DataFrame) -> Tuple[Dict[str, List[int]], Dict[Tuple[int, int], List[int]]]:
    """Build lookup dictionaries for address and truncated coordinates.

    For efficiency, this function constructs two dictionaries from the
    DataFrame of approved records (``"HoÃ n thÃ nh"`` status):

    * ``addr_dict`` maps each normalized address key to a list of DataFrame
      indices containing that address.
    * ``coord_dict`` maps each coordinate rounded to 6 decimal places
      (represented as integers to avoid floatingâ€point key issues) to a list
      of DataFrame indices containing coordinates falling into that bucket.

    Parameters
    ----------
    df: pd.DataFrame
        A DataFrame containing the approved records.

    Returns
    -------
    Tuple[Dict[str, List[int]], Dict[Tuple[int, int], List[int]]]
        The address dictionary and coordinate dictionary, respectively.
    """
    addr_dict: Dict[str, List[int]] = {}
    coord_dict: Dict[Tuple[int, int], List[int]] = {}
    for idx, row in df.iterrows():
        # Build address key
        addr_key = standardize_address(row)
        addr_dict.setdefault(addr_key, []).append(idx)

        # Build coordinate key (rounded to 6 decimals, scaled to integers)
        coord = parse_coords(row.get("Tá»a Ä‘á»™"))
        if coord:
            lat, lon = coord
            # Multiply by 1e6 and round to int to avoid float precision issues
            lat_key = int(round(lat * 1_000_000))
            lon_key = int(round(lon * 1_000_000))
            coord_dict.setdefault((lat_key, lon_key), []).append(idx)
    return addr_dict, coord_dict


def coordinate_match(
    pd_coord: str, ht_coords: List[str], threshold: float = 1e-6
) -> Optional[str]:
    """Determine whether a proposed coordinate matches any approved coordinate.

    Two matching strategies are considered:

    1. **Exact match**: both latitude and longitude values differ by no more
       than ``threshold``.
    2. **Prefix match**: either coordinate string is a prefix of the other
       (this covers cases where extra digits were appended to avoid detection).

    If a match is found, a descriptive reason string is returned.  If no
    match is found, ``None`` is returned.

    Parameters
    ----------
    pd_coord: str
        The coordinate string from the record under review ("PhÃª duyá»‡t").
    ht_coords: List[str]
        A list of coordinate strings from previously approved records with
        matching truncated values.
    threshold: float, optional
        The maximum absolute difference between latitudes and longitudes to
        consider an exact match.  Defaults to ``1e-6``.

    Returns
    -------
    Optional[str]
        A reason string if a match is found, otherwise ``None``.
    """
    candidate = parse_coords(pd_coord)
    if not candidate:
        return None
    pd_lat, pd_lon = candidate
    for ht_coord_str in ht_coords:
        ht = parse_coords(ht_coord_str)
        if not ht:
            continue
        ht_lat, ht_lon = ht
        # Exact match within threshold
        if abs(ht_lat - pd_lat) <= threshold and abs(ht_lon - pd_lon) <= threshold:
            return "Tá»a Ä‘á»™ trÃ¹ng 100%"
        # Prefix match: check if either string starts with the other
        if pd_coord and ht_coord_str:
            a = pd_coord.strip()
            b = ht_coord_str.strip()
            if a.startswith(b) or b.startswith(a):
                return "Tá»a Ä‘á»™ trÃ¹ng gáº§n chÃ­nh xÃ¡c (khá»›p theo tiá»n tá»‘)"
    return None


def check_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """Check for duplicate records between "PhÃª duyá»‡t" and "HoÃ n thÃ nh" entries.

    The function filters records in the "PhÃª duyá»‡t" stage and compares them
    against records in the "HoÃ n thÃ nh" stage.  A duplicate is flagged if
    either:

    * All five address fields (province/city, district/town, ward, street,
      house number) match exactly, or
    * The coordinates match exactly or approximately as defined in
      :func:`coordinate_match`.

    For each flagged record, the function includes the relevant address
    components, coordinates and the reasons for duplication.

    Parameters
    ----------
    df: pd.DataFrame
        A DataFrame containing the full exported dataset.

    Returns
    -------
    pd.DataFrame
        A DataFrame of flagged duplicate records with the following columns:

        * ``ID`` â€“ the record identifier.
        * ``Tá»‰nh/ThÃ nh phá»‘``, ``Quáº­n/Huyá»‡n/Thá»‹ xÃ£``, ``XÃ£/PhÆ°á»ng``,
          ``ÄÆ°á»ng/Phá»‘``, ``Sá»‘ nhÃ `` â€“ the address components.
        * ``Tá»a Ä‘á»™`` â€“ the coordinate string.
        * ``LÃ½ do trÃ¹ng`` â€“ a commaâ€separated list of reasons for flagging.
    """
    # Separate "HoÃ n thÃ nh" and "PhÃª duyá»‡t"
    ht_df = df[df["Giai Ä‘oáº¡n hiá»‡n táº¡i"] == "HoÃ n thÃ nh"].copy()
    pd_df = df[df["Giai Ä‘oáº¡n hiá»‡n táº¡i"] == "PhÃª duyá»‡t"].copy()

    # Build lookups from approved records
    addr_dict, coord_dict = build_lookup(ht_df)

    # Precompute coordinate strings for each index in the approved set
    ht_coord_strings: Dict[int, str] = {
        idx: str(ht_df.loc[idx, "Tá»a Ä‘á»™"]) for idx in ht_df.index
    }

    flagged: List[Dict[str, object]] = []
    for idx, row in pd_df.iterrows():
        reasons: List[str] = []
        addr_key = standardize_address(row)
        # Address match
        if addr_key in addr_dict:
            reasons.append(
                "TrÃ¹ng 5 thÃ´ng tin Ä‘á»‹a chá»‰ (Tá»‰nh/ThÃ nh phá»‘, Quáº­n/Huyá»‡n/Thá»‹ xÃ£, "
                "XÃ£/PhÆ°á»ng, ÄÆ°á»ng/Phá»‘, Sá»‘ nhÃ )"
            )
        # Coordinate match
        coord = row.get("Tá»a Ä‘á»™")
        coord_parsed = parse_coords(coord)
        if coord_parsed:
            # Round the candidate coordinates to build the key
            lat, lon = coord_parsed
            lat_key = int(round(lat * 1_000_000))
            lon_key = int(round(lon * 1_000_000))
            ht_indices = coord_dict.get((lat_key, lon_key), [])
            if ht_indices:
                ht_coords_list = [ht_coord_strings[i] for i in ht_indices]
                coord_reason = coordinate_match(str(coord), ht_coords_list)
                if coord_reason:
                    reasons.append(coord_reason)
        # If any reasons, record the duplicate
        if reasons:
            flagged.append(
                {
                    "ID": row.get("ID"),
                    "Tá»‰nh/ThÃ nh phá»‘": row.get("Tá»‰nh/ThÃ nh phá»‘"),
                    "Quáº­n/Huyá»‡n/Thá»‹ xÃ£": row.get("Quáº­n/Huyá»‡n/Thá»‹ xÃ£"),
                    "XÃ£/PhÆ°á»ng": row.get("XÃ£/PhÆ°á»ng"),
                    "ÄÆ°á»ng/Phá»‘": row.get("ÄÆ°á»ng/Phá»‘"),
                    "Sá»‘ nhÃ ": row.get("Sá»‘ nhÃ "),
                    "Tá»a Ä‘á»™": row.get("Tá»a Ä‘á»™"),
                    "LÃ½ do trÃ¹ng": ", ".join(sorted(set(reasons))),
                }
            )
    return pd.DataFrame(flagged)


def main() -> None:
    """Entry point for the Streamlit application."""
    if st is None:
        raise RuntimeError(
            "Streamlit khÃ´ng Ä‘Æ°á»£c cÃ i Ä‘áº·t. Vui lÃ²ng cÃ i Ä‘áº·t streamlit Ä‘á»ƒ cháº¡y á»©ng dá»¥ng."
        )

    st.set_page_config(
        page_title="Kiá»ƒm tra há»“ sÆ¡ trÃ¹ng", page_icon="ğŸ”", layout="wide"
    )
    st.title("ğŸ” CÃ´ng cá»¥ kiá»ƒm tra há»“ sÆ¡ trÃ¹ng trÆ°á»›c khi phÃª duyá»‡t")
    st.markdown(
        """
        ### HÆ°á»›ng dáº«n sá»­ dá»¥ng

        1. Chá»n loáº¡i kiá»ƒm tra (Äáº¥t á»Ÿ hoáº·c CÄƒn há»™ chung cÆ°).
        2. Táº£i lÃªn file Excel chá»©a dá»¯ liá»‡u xuáº¥t theo máº«u há»‡ thá»‘ng.
        3. á»¨ng dá»¥ng sáº½ lá»c cÃ¡c há»“ sÆ¡ Ä‘ang á»Ÿ giai Ä‘oáº¡n **PhÃª duyá»‡t** vÃ  so sÃ¡nh
           vá»›i cÃ¡c há»“ sÆ¡ Ä‘Ã£ **HoÃ n thÃ nh** Ä‘á»ƒ phÃ¡t hiá»‡n trÃ¹ng láº·p theo quy táº¯c:
           * TrÃ¹ng toÃ n bá»™ 5 thÃ´ng tin Ä‘á»‹a chá»‰: **Tá»‰nh/ThÃ nh phá»‘**, **Quáº­n/Huyá»‡n/Thá»‹ xÃ£**, **XÃ£/PhÆ°á»ng**, **ÄÆ°á»ng/Phá»‘**, **Sá»‘ nhÃ **.
           * TrÃ¹ng tá»a Ä‘á»™ chÃ­nh xÃ¡c hoáº·c trÃ¹ng gáº§n chÃ­nh xÃ¡c (vÃ­ dá»¥: "12.670322,108.101062"
             vÃ  "12.6703222,108.1010623").
        4. CÃ¡c há»“ sÆ¡ nghi ngá» trÃ¹ng sáº½ Ä‘Æ°á»£c liá»‡t kÃª kÃ¨m lÃ½ do Ä‘á»ƒ cÃ¡n bá»™ kiá»ƒm soÃ¡t xem xÃ©t.
        """
    )

    # Select the type of asset
    asset_type = st.radio(
        "Chá»n loáº¡i kiá»ƒm tra:", ["Äáº¥t á»Ÿ", "CÄƒn há»™ chung cÆ°"], index=0
    )
    uploaded_file = st.file_uploader(
        "Táº£i lÃªn file Excel xuáº¥t tá»« há»‡ thá»‘ng", type=["xlsx"]
    )
    if uploaded_file is not None:
        try:
            # Read Excel file
            df = pd.read_excel(uploaded_file)
        except Exception as e:
            st.error(f"KhÃ´ng thá»ƒ Ä‘á»c file Excel: {e}")
            return
        # Filter by asset type (if necessary)
        # Currently we only implement logic for 'Äáº¥t á»Ÿ'
        if asset_type == "Äáº¥t á»Ÿ":
            with st.spinner("Äang kiá»ƒm tra há»“ sÆ¡ trÃ¹ng..."):
                result_df = check_duplicates(df)
            st.success(
                f"ÄÃ£ phÃ¡t hiá»‡n {len(result_df)} há»“ sÆ¡ trÃ¹ng trong tá»•ng sá»‘ "
                f"{len(df[df['Giai Ä‘oáº¡n hiá»‡n táº¡i'] == 'PhÃª duyá»‡t'])} há»“ sÆ¡ Ä‘ang chá» phÃª duyá»‡t."
            )
            if not result_df.empty:
                st.dataframe(result_df)
                # Offer download
                csv = result_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    label="Táº£i danh sÃ¡ch trÃ¹ng (CSV)",
                    data=csv,
                    file_name="ho_so_trung.csv",
                    mime="text/csv",
                )
            else:
                st.info(
                    "KhÃ´ng phÃ¡t hiá»‡n há»“ sÆ¡ trÃ¹ng theo quy táº¯c hiá»‡n táº¡i."
                )
        else:
            st.warning(
                "Chá»©c nÄƒng kiá»ƒm tra CÄƒn há»™ chung cÆ° Ä‘ang Ä‘Æ°á»£c phÃ¡t triá»ƒn. Vui lÃ²ng chá»n Äáº¥t á»Ÿ."
            )


if __name__ == "__main__":
    main()
