"""
duplicate_checker.py

C√¥ng c·ª• Streamlit ƒë·ªÉ ki·ªÉm tra tr√πng h·ªì s∆° tr∆∞·ªõc khi ph√™ duy·ªát.
H·ªó tr·ª£ 2 lo·∫°i:
- ƒê·∫§T ·ªû (Land)
- CHUNG C∆Ø (Apartment)

ƒê·∫§T ·ªû: gi·ªØ nguy√™n logic hi·ªán t·∫°i c·ªßa b·∫°n.

CHUNG C∆Ø: rule check tr√πng theo:
- T·ªânh/Th√†nh ph·ªë (c·ªôt W)
- D·ª± √°n/Khu ƒë√¥ th·ªã/Khu ph√¢n l√¥ (c·ªôt AB)
- ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n (c·ªôt AD)

Output:
- ID
- Ng∆∞·ªùi t·∫°o
- L√Ω do tr√πng
- ƒê·ªãa ch·ªâ/T·ªça ƒë·ªô tr√πng
- ID tr√πng
- Ng∆∞·ªùi t·∫°o tr√πng
"""

from __future__ import annotations

from typing import Optional, List, Dict, Any, Set
import io

import pandas as pd

try:
    import streamlit as st  # type: ignore
except ImportError:  # pragma: no cover
    st = None  # type: ignore


# ==========================
#  Constants
# ==========================

# ---- ƒê·∫§T ·ªû (gi·ªØ nguy√™n)
ADDR_COLS = [
    "T·ªânh/Th√†nh ph·ªë",       # W
    "Qu·∫≠n/Huy·ªán/Th·ªã x√£",    # X
    "X√£/Ph∆∞·ªùng",            # Y
    "ƒê∆∞·ªùng/Ph·ªë",            # Z
    "S·ªë nh√†",               # AE
]

# ---- CHUNG C∆Ø (m·ªõi)
CHUNGCU_COLS = [
    "T·ªânh/Th√†nh ph·ªë",                 # W
    "D·ª± √°n/Khu ƒë√¥ th·ªã/Khu ph√¢n l√¥",    # AB
    "ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n",              # AD
]

CREATOR_COL = "Ng∆∞·ªùi t·∫°o"                     # c·ªôt E
TIME_COL = "Th·ªùi ƒëi·ªÉm thu th·∫≠p th√¥ng tin"     # c·ªôt L
COORD_COL = "T·ªça ƒë·ªô"                          # c·ªôt AF
STATUS_COL = "Giai ƒëo·∫°n hi·ªán t·∫°i"             # c·ªôt H
ID_COL = "ID"


# ==========================
#  Helpers
# ==========================

def build_addr_key(row: pd.Series) -> str:
    parts = [str(row.get(col, "")).strip().lower() for col in ADDR_COLS]
    return "||".join(parts)


def build_chungcu_key(row: pd.Series) -> str:
    parts = [str(row.get(col, "")).strip().lower() for col in CHUNGCU_COLS]
    return "||".join(parts)


def normalize_coord(value: Any, max_len: int = 8) -> Optional[str]:
    """Chu·∫©n h√≥a t·ªça ƒë·ªô ƒë·ªÉ b·∫Øt c·∫£ case th√™m s·ªë l·∫ª ph√≠a sau."""
    if pd.isna(value):
        return None
    try:
        lat_raw, lon_raw = str(value).split(",")
        lat = lat_raw.strip()[:max_len]
        lon = lon_raw.strip()[:max_len]
        return f"{lat},{lon}"
    except Exception:
        return None


def format_address(row: pd.Series) -> str:
    """S·ªë nh√† ‚Äì ƒê∆∞·ªùng/Ph·ªë ‚Äì X√£/Ph∆∞·ªùng ‚Äì Qu·∫≠n/Huy·ªán/Th·ªã x√£ ‚Äì T·ªânh/Th√†nh ph·ªë."""
    parts = [
        str(row.get("S·ªë nh√†", "")).strip(),
        str(row.get("ƒê∆∞·ªùng/Ph·ªë", "")).strip(),
        str(row.get("X√£/Ph∆∞·ªùng", "")).strip(),
        str(row.get("Qu·∫≠n/Huy·ªán/Th·ªã x√£", "")).strip(),
        str(row.get("T·ªânh/Th√†nh ph·ªë", "")).strip(),
    ]
    return " ‚Äì ".join([p for p in parts if p])


def format_chungcu_info(row: pd.Series) -> str:
    """D·ª± √°n/Khu ƒë√¥ th·ªã/Khu ph√¢n l√¥ ‚Äì ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n ‚Äì T·ªânh/Th√†nh ph·ªë."""
    parts = [
        str(row.get("D·ª± √°n/Khu ƒë√¥ th·ªã/Khu ph√¢n l√¥", "")).strip(),
        str(row.get("ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n", "")).strip(),
        str(row.get("T·ªânh/Th√†nh ph·ªë", "")).strip(),
    ]
    return " ‚Äì ".join([p for p in parts if p])


# ==========================
#  Core Logic (ƒê·∫§T ·ªû - gi·ªØ nguy√™n)
# ==========================

def _prepare(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """Chu·∫©n h√≥a chung: key ƒë·ªãa ch·ªâ, t·ªça ƒë·ªô, th·ªùi gian; t√°ch nh√≥m tr·∫°ng th√°i."""
    required_cols = ADDR_COLS + [CREATOR_COL, TIME_COL, COORD_COL, STATUS_COL, ID_COL]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Thi·∫øu c·ªôt: {col}")

    df = df.copy()
    df["addr_key"] = df.apply(build_addr_key, axis=1)
    df["coord_norm"] = df[COORD_COL].apply(normalize_coord)
    df["time_norm"] = pd.to_datetime(df[TIME_COL], dayfirst=True, errors="coerce")
    df["creator_norm"] = df[CREATOR_COL].astype(str).str.strip()

    hoan_thanh = df[df[STATUS_COL] == "Ho√†n th√†nh"].copy()
    phe_duyet = df[df[STATUS_COL] == "Ph√™ duy·ªát"].copy()

    return {"all": df, "hoan_thanh": hoan_thanh, "phe_duyet": phe_duyet}


def _build_groups(hoan_thanh: pd.DataFrame):
    """T·∫°o group cho Ho√†n th√†nh ƒë·ªÉ t√°i s·ª≠ d·ª•ng."""
    addr_groups = hoan_thanh.groupby("addr_key").groups
    coord_groups = hoan_thanh.groupby("coord_norm").groups
    return addr_groups, coord_groups


def _collect_result(
    row: pd.Series,
    duplicate_ids: Set[Any],
    duplicate_creators: Set[str],
    has_coord_dup: bool,
    has_addr_dup: bool,
    severity_label: str,
    reason_details: List[str],
) -> Dict[str, Any]:
    if has_coord_dup:
        info = f"T·ªça ƒë·ªô: {row.get(COORD_COL, '')}"
    elif has_addr_dup:
        info = f"ƒê·ªãa ch·ªâ: {format_address(row)}"
    else:
        info = ""

    return {
        "ID": row.get(ID_COL),
        "Ng∆∞·ªùi t·∫°o": row.get("creator_norm", ""),
        "L√Ω do tr√πng": f"{severity_label} ‚Äì " + " ; ".join(reason_details),
        "ƒê·ªãa ch·ªâ/T·ªça ƒë·ªô tr√πng": info,
        "ID tr√πng": "; ".join(str(x) for x in sorted(duplicate_ids)),
        "Ng∆∞·ªùi t·∫°o tr√πng": "; ".join(sorted(duplicate_creators)),
    }


def check_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    (ƒê·∫§T ·ªû) Tr·∫£ v·ªÅ b·∫£ng tr√πng bao g·ªìm:
    - Ph√™ duy·ªát vs Ho√†n th√†nh
    - Ho√†n th√†nh vs Ho√†n th√†nh
    """
    prep = _prepare(df)
    hoan_thanh = prep["hoan_thanh"]
    phe_duyet = prep["phe_duyet"]

    addr_groups, coord_groups = _build_groups(hoan_thanh)

    results: List[Dict[str, Any]] = []

    # ==========================
    # A. Ph√™ duy·ªát vs Ho√†n th√†nh
    # ==========================
    for _, row in phe_duyet.iterrows():
        duplicate_ids: Set[Any] = set()
        duplicate_creators: Set[str] = set()
        reason_details: List[str] = []
        severity: Optional[str] = None

        row_time = row["time_norm"]
        addr_key = row["addr_key"]
        coord_key = row["coord_norm"]
        creator = row["creator_norm"]

        has_addr_dup = False
        has_coord_dup = False

        # ---- Tr√πng ƒë·ªãa ch·ªâ
        addr_idx = addr_groups.get(addr_key)
        if addr_idx is not None and len(addr_idx) > 0:
            subset = hoan_thanh.loc[addr_idx]
            subset = subset[subset["time_norm"] < row_time]

            if not subset.empty:
                has_addr_dup = True
                # c√πng ng∆∞·ªùi t·∫°o / kh√°c ng∆∞·ªùi t·∫°o
                same_creator = subset[subset["creator_norm"] == creator]
                diff_creator = subset[subset["creator_norm"] != creator]

                if not same_creator.empty:
                    severity = "C·∫£nh b√°o tr√πng"
                    duplicate_ids.update(same_creator[ID_COL])
                    duplicate_creators.update(same_creator["creator_norm"])
                    reason_details.append(
                        "Ph√™ duy·ªát vs Ho√†n th√†nh ‚Äì C√πng Ng∆∞·ªùi t·∫°o v√† tr√πng 5 th√¥ng tin ƒë·ªãa ch·ªâ"
                    )

                if not diff_creator.empty:
                    if severity is None:
                        severity = "Nghi ng·ªù tr√πng"
                    duplicate_ids.update(diff_creator[ID_COL])
                    duplicate_creators.update(diff_creator["creator_norm"])
                    reason_details.append(
                        "Ph√™ duy·ªát vs Ho√†n th√†nh ‚Äì Kh√°c Ng∆∞·ªùi t·∫°o nh∆∞ng tr√πng 5 th√¥ng tin ƒë·ªãa ch·ªâ"
                    )

        # ---- Tr√πng t·ªça ƒë·ªô
        coord_idx = coord_groups.get(coord_key)
        if coord_idx is not None and len(coord_idx) > 0:
            subset = hoan_thanh.loc[coord_idx]
            subset = subset[subset["time_norm"] < row_time]

            if not subset.empty:
                has_coord_dup = True
                severity = "C·∫£nh b√°o tr√πng"
                duplicate_ids.update(subset[ID_COL])
                duplicate_creators.update(subset["creator_norm"])
                reason_details.append(
                    "Ph√™ duy·ªát vs Ho√†n th√†nh ‚Äì Tr√πng t·ªça ƒë·ªô (100% ho·∫∑c g·∫ßn ƒë√∫ng)"
                )

        if duplicate_ids:
            if severity is None:
                severity = "Nghi ng·ªù tr√πng"
            results.append(
                _collect_result(
                    row=row,
                    duplicate_ids=duplicate_ids,
                    duplicate_creators=duplicate_creators,
                    has_coord_dup=has_coord_dup,
                    has_addr_dup=has_addr_dup,
                    severity_label=severity,
                    reason_details=reason_details,
                )
            )

    # ==========================
    # B. Ho√†n th√†nh vs Ho√†n th√†nh
    # ==========================
    for _, row in hoan_thanh.iterrows():
        duplicate_ids: Set[Any] = set()
        duplicate_creators: Set[str] = set()
        reason_details: List[str] = []
        severity: Optional[str] = None

        row_time = row["time_norm"]
        addr_key = row["addr_key"]
        coord_key = row["coord_norm"]
        creator = row["creator_norm"]
        row_id = row[ID_COL]

        has_addr_dup = False
        has_coord_dup = False

        # ---- Tr√πng ƒë·ªãa ch·ªâ gi·ªØa Ho√†n th√†nh v·ªõi nhau
        addr_idx = addr_groups.get(addr_key)
        if addr_idx is not None and len(addr_idx) > 0:
            subset = hoan_thanh.loc[addr_idx]
            subset = subset[(subset["time_norm"] < row_time) & (subset[ID_COL] != row_id)]

            if not subset.empty:
                has_addr_dup = True
                same_creator = subset[subset["creator_norm"] == creator]
                diff_creator = subset[subset["creator_norm"] != creator]

                if not same_creator.empty:
                    severity = "C·∫£nh b√°o tr√πng"
                    duplicate_ids.update(same_creator[ID_COL])
                    duplicate_creators.update(same_creator["creator_norm"])
                    reason_details.append(
                        "Ho√†n th√†nh vs Ho√†n th√†nh ‚Äì C√πng Ng∆∞·ªùi t·∫°o v√† tr√πng 5 th√¥ng tin ƒë·ªãa ch·ªâ"
                    )

                if not diff_creator.empty:
                    if severity is None:
                        severity = "Nghi ng·ªù tr√πng"
                    duplicate_ids.update(diff_creator[ID_COL])
                    duplicate_creators.update(diff_creator["creator_norm"])
                    reason_details.append(
                        "Ho√†n th√†nh vs Ho√†n th√†nh ‚Äì Kh√°c Ng∆∞·ªùi t·∫°o nh∆∞ng tr√πng 5 th√¥ng tin ƒë·ªãa ch·ªâ"
                    )

        # ---- Tr√πng t·ªça ƒë·ªô gi·ªØa Ho√†n th√†nh v·ªõi nhau
        coord_idx = coord_groups.get(coord_key)
        if coord_idx is not None and len(coord_idx) > 0:
            subset = hoan_thanh.loc[coord_idx]
            subset = subset[(subset["time_norm"] < row_time) & (subset[ID_COL] != row_id)]

            if not subset.empty:
                has_coord_dup = True
                severity = "C·∫£nh b√°o tr√πng"
                duplicate_ids.update(subset[ID_COL])
                duplicate_creators.update(subset["creator_norm"])
                reason_details.append(
                    "Ho√†n th√†nh vs Ho√†n th√†nh ‚Äì Tr√πng t·ªça ƒë·ªô (100% ho·∫∑c g·∫ßn ƒë√∫ng)"
                )

        if duplicate_ids:
            if severity is None:
                severity = "Nghi ng·ªù tr√πng"
            results.append(
                _collect_result(
                    row=row,
                    duplicate_ids=duplicate_ids,
                    duplicate_creators=duplicate_creators,
                    has_coord_dup=has_coord_dup,
                    has_addr_dup=has_addr_dup,
                    severity_label=severity,
                    reason_details=reason_details,
                )
            )

    return pd.DataFrame(results)


# ==========================
#  Core Logic (CHUNG C∆Ø - m·ªõi)
# ==========================

def check_duplicates_chungcu(df: pd.DataFrame) -> pd.DataFrame:
    """
    (CHUNG C∆Ø) Check tr√πng theo:
    - T·ªânh/Th√†nh ph·ªë (W)
    - D·ª± √°n/Khu ƒë√¥ th·ªã/Khu ph√¢n l√¥ (AB)
    - ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n (AD)

    M·ª©c ƒë·ªô:
    - C·∫£nh b√°o tr√πng: c√≥ √≠t nh·∫•t 1 b·∫£n ghi tr√πng tr∆∞·ªõc ƒë√≥ c√πng Ng∆∞·ªùi t·∫°o
    - Nghi ng·ªù tr√πng: ch·ªâ tr√πng v·ªõi Ng∆∞·ªùi t·∫°o kh√°c
    """
    required_cols = CHUNGCU_COLS + [CREATOR_COL, ID_COL]
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Thi·∫øu c·ªôt: {col}")

    df = df.copy()
    df["creator_norm"] = df[CREATOR_COL].astype(str).str.strip()
    df["chungcu_key"] = df.apply(build_chungcu_key, axis=1)

    # b·ªè c√°c d√≤ng key r·ªóng ho√†n to√†n
    valid_mask = df["chungcu_key"].str.replace("|", "", regex=False).str.strip().ne("")
    df = df[valid_mask].copy()

    # group theo key
    groups = df.groupby("chungcu_key", sort=False).groups

    results: List[Dict[str, Any]] = []

    for key, idx in groups.items():
        if idx is None or len(idx) <= 1:
            continue

        group_df = df.loc[idx].copy()

        # duy·ªát t·ª´ng row v√† coi c√°c row kh√°c trong group l√† "tr√πng"
        for _, row in group_df.iterrows():
            row_id = row[ID_COL]
            creator = row["creator_norm"]

            others = group_df[group_df[ID_COL] != row_id]
            if others.empty:
                continue

            same_creator = others[others["creator_norm"] == creator]
            diff_creator = others[others["creator_norm"] != creator]

            if not same_creator.empty:
                severity = "C·∫£nh b√°o tr√πng"
                reason = "Chung c∆∞ ‚Äì Tr√πng T·ªânh + D·ª± √°n/KƒêT/Khu ph√¢n l√¥ + ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n (c√πng Ng∆∞·ªùi t·∫°o)"
            else:
                severity = "Nghi ng·ªù tr√πng"
                reason = "Chung c∆∞ ‚Äì Tr√πng T·ªânh + D·ª± √°n/KƒêT/Khu ph√¢n l√¥ + ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n (kh√°c Ng∆∞·ªùi t·∫°o)"

            duplicate_ids = set(others[ID_COL].tolist())
            duplicate_creators = set(others["creator_norm"].tolist())

            results.append({
                "ID": row_id,
                "Ng∆∞·ªùi t·∫°o": creator,
                "L√Ω do tr√πng": f"{severity} ‚Äì {reason}",
                "ƒê·ªãa ch·ªâ/T·ªça ƒë·ªô tr√πng": f"Chung c∆∞: {format_chungcu_info(row)}",
                "ID tr√πng": "; ".join(str(x) for x in sorted(duplicate_ids)),
                "Ng∆∞·ªùi t·∫°o tr√πng": "; ".join(sorted(duplicate_creators)),
            })

    return pd.DataFrame(results)


# ==========================
#  Streamlit App
# ==========================

def run_app() -> None:  # pragma: no cover
    if st is None:
        raise RuntimeError("Streamlit ch∆∞a ƒë∆∞·ª£c c√†i. Ch·∫°y: pip install streamlit")

    st.set_page_config(page_title="iFast Duplicate Checker", layout="wide")
    st.title("üßÆ iFast ‚Äì C√¥ng c·ª• ki·ªÉm tra tr√πng h·ªì s∆°")

    asset_type = st.radio(
        "Ch·ªçn lo·∫°i h·ªì s∆° ki·ªÉm tra",
        ["ƒê·∫•t ·ªü", "Chung c∆∞"],
        horizontal=True
    )

    if asset_type == "ƒê·∫•t ·ªü":
        st.markdown(
            """
            C√¥ng c·ª• ki·ªÉm tra tr√πng **h·ªì s∆° ƒê·∫•t ·ªü** trong iFast.

            **Nh√≥m ki·ªÉm tra:**
            - Ph√™ duy·ªát vs Ho√†n th√†nh (h·ªì s∆° ƒëang tr√¨nh so v·ªõi h·ªì s∆° ƒë√£ ho√†n th√†nh)
            - Ho√†n th√†nh vs Ho√†n th√†nh (c√°c h·ªì s∆° ƒë√£ ho√†n th√†nh tr√πng nhau)

            **∆Øu ti√™n hi·ªÉn th·ªã:**
            - N·∫øu tr√πng t·ªça ƒë·ªô ‚Üí ch·ªâ hi·ªÉn th·ªã t·ªça ƒë·ªô
            - N·∫øu ch·ªâ tr√πng ƒë·ªãa ch·ªâ ‚Üí hi·ªÉn th·ªã ƒë·ªãa ch·ªâ
            """
        )
    else:
        st.markdown(
            """
            C√¥ng c·ª• ki·ªÉm tra tr√πng **h·ªì s∆° Chung c∆∞** trong iFast.

            **Rule check tr√πng:**
            - T·ªânh/Th√†nh ph·ªë (W)
            - D·ª± √°n/Khu ƒë√¥ th·ªã/Khu ph√¢n l√¥ (AB)
            - ƒê·ªãa ch·ªâ cƒÉn h·ªô/s√†n (AD)

            **M·ª©c ƒë·ªô:**
            - C√πng Ng∆∞·ªùi t·∫°o ‚Üí C·∫£nh b√°o tr√πng
            - Kh√°c Ng∆∞·ªùi t·∫°o ‚Üí Nghi ng·ªù tr√πng
            """
        )

    uploaded = st.file_uploader("üì• T·∫£i file Excel (.xlsx) xu·∫•t t·ª´ iFast", type=["xlsx"])
    if uploaded is None:
        st.info("Vui l√≤ng t·∫£i l√™n file Excel ƒë·ªÉ b·∫Øt ƒë·∫ßu ki·ªÉm tra.")
        return

    try:
        df = pd.read_excel(uploaded)
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc file Excel: {e}")
        return

    st.subheader("üîç Xem tr∆∞·ªõc d·ªØ li·ªáu")
    with st.expander("Xem 5 d√≤ng ƒë·∫ßu"):
        st.dataframe(df.head())

    st.subheader("üìä K·∫øt qu·∫£ ki·ªÉm tra tr√πng")

    try:
        if asset_type == "ƒê·∫•t ·ªü":
            dup_df = check_duplicates(df)
        else:
            dup_df = check_duplicates_chungcu(df)
    except Exception as e:
        st.error(f"L·ªói khi ki·ªÉm tra tr√πng: {e}")
        return

    if dup_df.empty:
        st.success("‚úÖ Kh√¥ng ph√°t hi·ªán h·ªì s∆° tr√πng ho·∫∑c nghi ng·ªù tr√πng.")
    else:
        st.error(f"‚ö† Ph√°t hi·ªán {len(dup_df)} h·ªì s∆° tr√πng ho·∫∑c nghi ng·ªù tr√πng.")
        st.dataframe(dup_df, use_container_width=True)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            dup_df.to_excel(writer, index=False, sheet_name="Duplicates")
        output.seek(0)

        st.download_button(
            label="‚¨áÔ∏è T·∫£i danh s√°ch tr√πng (Excel)",
            data=output,
            file_name="detected_duplicates.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )


if __name__ == "__main__":  # pragma: no cover
    if st is not None:
        run_app()
    else:
        print("ƒê√¢y l√† module cho Streamlit. Ch·∫°y b·∫±ng:\n  streamlit run duplicate_checker.py")
